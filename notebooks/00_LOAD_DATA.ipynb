{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43b1a101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "                    UIDAI HACKATHON\n",
      "               DATA LOADING PIPELINE\n",
      "======================================================================\n",
      "\n",
      " Base folder: c:\\Users\\ahmad\\Desktop\\hackathon\n",
      " Folders ready:\n",
      "   - data/processed/\n",
      "   - outputs/figures/\n",
      "\n",
      "======================================================================\n",
      " LOADING: ENROLLMENT DATA\n",
      "======================================================================\n",
      "Folder: c:\\Users\\ahmad\\Desktop\\hackathon\\data\\raw\\enrollment\n",
      " Found 3 CSV files\n",
      "\n",
      "  [1/3] file1.csv...  500,000 rows\n",
      "  [2/3] file2.csv...  500,000 rows\n",
      "  [3/3] file3.csv...  6,029 rows\n",
      "\n",
      " Combining 3 files...\n",
      " SUCCESS!\n",
      "   Total rows: 1,006,029\n",
      "   Total columns: 7\n",
      "   Memory: 199.1 MB\n",
      "\n",
      " Saving to: c:\\Users\\ahmad\\Desktop\\hackathon\\data\\processed\\enrollment_combined.csv\n",
      " Saved!\n",
      "\n",
      " ENROLLMENT COLUMNS (7 total):\n",
      "    1. date                           | object     | 1,006,029 non-null\n",
      "    2. state                          | object     | 1,006,029 non-null\n",
      "    3. district                       | object     | 1,006,029 non-null\n",
      "    4. pincode                        | int64      | 1,006,029 non-null\n",
      "    5. age_0_5                        | int64      | 1,006,029 non-null\n",
      "    6. age_5_17                       | int64      | 1,006,029 non-null\n",
      "    7. age_18_greater                 | int64      | 1,006,029 non-null\n",
      "\n",
      " SAMPLE DATA (first 3 rows):\n",
      "         date          state          district  pincode  age_0_5  age_5_17  age_18_greater\n",
      "0  02-03-2025      Meghalaya  East Khasi Hills   793121       11        61              37\n",
      "1  09-03-2025      Karnataka   Bengaluru Urban   560043       14        33              39\n",
      "2  09-03-2025  Uttar Pradesh      Kanpur Nagar   208001       29        82              12\n",
      "\n",
      " BASIC STATS:\n",
      "   Date range: 01-04-2025 to 31-12-2025\n",
      "   Missing values: 0\n",
      "   Duplicate rows: 22,957\n",
      "\n",
      "======================================================================\n",
      " LOADING: DEMOGRAPHIC UPDATE DATA\n",
      "======================================================================\n",
      "Folder: c:\\Users\\ahmad\\Desktop\\hackathon\\data\\raw\\demographic\n",
      " Found 5 CSV files\n",
      "\n",
      "  [1/5] file1.csv...  500,000 rows\n",
      "  [2/5] file2.csv...  500,000 rows\n",
      "  [3/5] file3.csv...  500,000 rows\n",
      "  [4/5] file4.csv...  500,000 rows\n",
      "  [5/5] file5.csv...  71,700 rows\n",
      "\n",
      " Combining 5 files...\n",
      " SUCCESS!\n",
      "   Total rows: 2,071,700\n",
      "   Total columns: 6\n",
      "   Memory: 394.8 MB\n",
      "\n",
      " Saving to: c:\\Users\\ahmad\\Desktop\\hackathon\\data\\processed\\demographic_combined.csv\n",
      " Saved!\n",
      "\n",
      " DEMOGRAPHIC COLUMNS (6 total):\n",
      "    1. date\n",
      "    2. state\n",
      "    3. district\n",
      "    4. pincode\n",
      "    5. demo_age_5_17\n",
      "    6. demo_age_17_\n",
      "\n",
      "======================================================================\n",
      " LOADING: BIOMETRIC UPDATE DATA\n",
      "======================================================================\n",
      "Folder: c:\\Users\\ahmad\\Desktop\\hackathon\\data\\raw\\biometric\n",
      " Found 4 CSV files\n",
      "\n",
      "  [1/4] file1.csv...  500,000 rows\n",
      "  [2/4] file2.csv...  500,000 rows\n",
      "  [3/4] file3.csv...  500,000 rows\n",
      "  [4/4] file4.csv...  361,108 rows\n",
      "\n",
      " Combining 4 files...\n",
      " SUCCESS!\n",
      "   Total rows: 1,861,108\n",
      "   Total columns: 6\n",
      "   Memory: 354.5 MB\n",
      "\n",
      " Saving to: c:\\Users\\ahmad\\Desktop\\hackathon\\data\\processed\\biometric_combined.csv\n",
      " Saved!\n",
      "\n",
      " BIOMETRIC COLUMNS (6 total):\n",
      "    1. date\n",
      "    2. state\n",
      "    3. district\n",
      "    4. pincode\n",
      "    5. bio_age_5_17\n",
      "    6. bio_age_17_\n",
      "\n",
      "======================================================================\n",
      "                         SUMMARY\n",
      "======================================================================\n",
      "\n",
      " ENROLLMENT DATA:\n",
      "   Files combined: 3\n",
      "   Total records: 1,006,029\n",
      "   Columns: 7\n",
      "\n",
      " DEMOGRAPHIC DATA:\n",
      "   Files combined: 5\n",
      "   Total records: 2,071,700\n",
      "   Columns: 6\n",
      "\n",
      " BIOMETRIC DATA:\n",
      "   Files combined: 4\n",
      "   Total records: 1,861,108\n",
      "   Columns: 6\n",
      "\n",
      "======================================================================\n",
      " DATA LOADING COMPLETE!\n",
      "======================================================================\n",
      "\n",
      " Column reference saved: c:\\Users\\ahmad\\Desktop\\hackathon\\data\\processed\\COLUMN_REFERENCE.txt\n",
      "\n",
      " NEXT STEP: Check the column names above and tell me!\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# UIDAI HACKATHON - DATA LOADING\n",
    "# Working with YOUR exact folder structure\n",
    "# ==========================================\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\" \"*20 + \"UIDAI HACKATHON\")\n",
    "print(\" \"*15 + \"DATA LOADING PIPELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ==========================================\n",
    "# SET BASE PATH (FROM NOTEBOOKS FOLDER)\n",
    "# ==========================================\n",
    "\n",
    "# Since we're running from notebooks/ folder\n",
    "# We need to go up one level to reach data/\n",
    "BASE_PATH = os.path.abspath('..')  # Go up to HACKATHON folder\n",
    "print(f\"\\n Base folder: {BASE_PATH}\")\n",
    "\n",
    "# ==========================================\n",
    "# CREATE OUTPUT FOLDERS IF NOT EXIST\n",
    "# ==========================================\n",
    "\n",
    "os.makedirs(os.path.join(BASE_PATH, 'data', 'processed'), exist_ok=True)\n",
    "os.makedirs(os.path.join(BASE_PATH, 'outputs', 'figures'), exist_ok=True)\n",
    "\n",
    "print(f\" Folders ready:\")\n",
    "print(f\"   - data/processed/\")\n",
    "print(f\"   - outputs/figures/\")\n",
    "\n",
    "# ==========================================\n",
    "# FUNCTION: COMBINE CSV FILES\n",
    "# ==========================================\n",
    "\n",
    "def combine_csv_files(folder_name, dataset_name):\n",
    "    \"\"\"\n",
    "    Combines all CSV files in a folder\n",
    "    folder_name: 'enrollment', 'demographic', or 'biometric'\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\" LOADING: {dataset_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Build path: HACKATHON/data/raw/folder_name/*.csv\n",
    "    folder_path = os.path.join(BASE_PATH, 'data', 'raw', folder_name)\n",
    "    print(f\"Folder: {folder_path}\")\n",
    "    \n",
    "    # Check if folder exists\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\" ERROR: Folder not found!\")\n",
    "        return None\n",
    "    \n",
    "    # Get all CSV files\n",
    "    csv_pattern = os.path.join(folder_path, '*.csv')\n",
    "    csv_files = glob.glob(csv_pattern)\n",
    "    \n",
    "    if len(csv_files) == 0:\n",
    "        print(f\" ERROR: No CSV files found!\")\n",
    "        print(f\"   Looking for: {csv_pattern}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\" Found {len(csv_files)} CSV files\\n\")\n",
    "    \n",
    "    # Load each file\n",
    "    dfs = []\n",
    "    total_rows = 0\n",
    "    \n",
    "    for i, file in enumerate(sorted(csv_files), 1):\n",
    "        filename = os.path.basename(file)\n",
    "        print(f\"  [{i}/{len(csv_files)}] {filename}...\", end=' ')\n",
    "        \n",
    "        try:\n",
    "            # Read CSV\n",
    "            df = pd.read_csv(file, low_memory=False)\n",
    "            rows = len(df)\n",
    "            total_rows += rows\n",
    "            print(f\" {rows:,} rows\")\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\" ERROR: {str(e)}\")\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    if len(dfs) > 0:\n",
    "        print(f\"\\n Combining {len(dfs)} files...\")\n",
    "        combined_df = pd.concat(dfs, ignore_index=True)\n",
    "        print(f\" SUCCESS!\")\n",
    "        print(f\"   Total rows: {len(combined_df):,}\")\n",
    "        print(f\"   Total columns: {len(combined_df.columns)}\")\n",
    "        \n",
    "        # Memory info\n",
    "        memory_mb = combined_df.memory_usage(deep=True).sum() / 1024**2\n",
    "        print(f\"   Memory: {memory_mb:.1f} MB\")\n",
    "        \n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\" No data loaded\")\n",
    "        return None\n",
    "\n",
    "# ==========================================\n",
    "# LOAD ENROLLMENT DATA (3 FILES)\n",
    "# ==========================================\n",
    "\n",
    "df_enrollment = combine_csv_files('enrollment', 'ENROLLMENT DATA')\n",
    "\n",
    "if df_enrollment is not None:\n",
    "    # Save combined file\n",
    "    output_path = os.path.join(BASE_PATH, 'data', 'processed', 'enrollment_combined.csv')\n",
    "    print(f\"\\n Saving to: {output_path}\")\n",
    "    df_enrollment.to_csv(output_path, index=False)\n",
    "    print(f\" Saved!\")\n",
    "    \n",
    "    # Show structure\n",
    "    print(f\"\\n ENROLLMENT COLUMNS ({len(df_enrollment.columns)} total):\")\n",
    "    for i, col in enumerate(df_enrollment.columns, 1):\n",
    "        dtype = df_enrollment[col].dtype\n",
    "        non_null = df_enrollment[col].notna().sum()\n",
    "        print(f\"   {i:2d}. {col:30s} | {str(dtype):10s} | {non_null:,} non-null\")\n",
    "    \n",
    "    print(f\"\\n SAMPLE DATA (first 3 rows):\")\n",
    "    print(df_enrollment.head(3).to_string())\n",
    "    \n",
    "    print(f\"\\n BASIC STATS:\")\n",
    "    print(f\"   Date range: {df_enrollment.iloc[:, 0].min()} to {df_enrollment.iloc[:, 0].max()}\")\n",
    "    print(f\"   Missing values: {df_enrollment.isnull().sum().sum():,}\")\n",
    "    print(f\"   Duplicate rows: {df_enrollment.duplicated().sum():,}\")\n",
    "\n",
    "# ==========================================\n",
    "# LOAD DEMOGRAPHIC DATA (5 FILES)\n",
    "# ==========================================\n",
    "\n",
    "df_demographic = combine_csv_files('demographic', 'DEMOGRAPHIC UPDATE DATA')\n",
    "\n",
    "if df_demographic is not None:\n",
    "    output_path = os.path.join(BASE_PATH, 'data', 'processed', 'demographic_combined.csv')\n",
    "    print(f\"\\n Saving to: {output_path}\")\n",
    "    df_demographic.to_csv(output_path, index=False)\n",
    "    print(f\" Saved!\")\n",
    "    \n",
    "    print(f\"\\n DEMOGRAPHIC COLUMNS ({len(df_demographic.columns)} total):\")\n",
    "    for i, col in enumerate(df_demographic.columns, 1):\n",
    "        print(f\"   {i:2d}. {col}\")\n",
    "\n",
    "# ==========================================\n",
    "# LOAD BIOMETRIC DATA (4 FILES)\n",
    "# ==========================================\n",
    "\n",
    "df_biometric = combine_csv_files('biometric', 'BIOMETRIC UPDATE DATA')\n",
    "\n",
    "if df_biometric is not None:\n",
    "    output_path = os.path.join(BASE_PATH, 'data', 'processed', 'biometric_combined.csv')\n",
    "    print(f\"\\n Saving to: {output_path}\")\n",
    "    df_biometric.to_csv(output_path, index=False)\n",
    "    print(f\" Saved!\")\n",
    "    \n",
    "    print(f\"\\n BIOMETRIC COLUMNS ({len(df_biometric.columns)} total):\")\n",
    "    for i, col in enumerate(df_biometric.columns, 1):\n",
    "        print(f\"   {i:2d}. {col}\")\n",
    "\n",
    "# ==========================================\n",
    "# FINAL SUMMARY\n",
    "# ==========================================\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\" \"*25 + \"SUMMARY\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "if df_enrollment is not None:\n",
    "    print(f\"\\n ENROLLMENT DATA:\")\n",
    "    print(f\"   Files combined: 3\")\n",
    "    print(f\"   Total records: {len(df_enrollment):,}\")\n",
    "    print(f\"   Columns: {len(df_enrollment.columns)}\")\n",
    "\n",
    "if df_demographic is not None:\n",
    "    print(f\"\\n DEMOGRAPHIC DATA:\")\n",
    "    print(f\"   Files combined: 5\")\n",
    "    print(f\"   Total records: {len(df_demographic):,}\")\n",
    "    print(f\"   Columns: {len(df_demographic.columns)}\")\n",
    "\n",
    "if df_biometric is not None:\n",
    "    print(f\"\\n BIOMETRIC DATA:\")\n",
    "    print(f\"   Files combined: 4\")\n",
    "    print(f\"   Total records: {len(df_biometric):,}\")\n",
    "    print(f\"   Columns: {len(df_biometric.columns)}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\" DATA LOADING COMPLETE!\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# ==========================================\n",
    "# SAVE COLUMN REFERENCE FILE\n",
    "# ==========================================\n",
    "\n",
    "ref_path = os.path.join(BASE_PATH, 'data', 'processed', 'COLUMN_REFERENCE.txt')\n",
    "with open(ref_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"UIDAI HACKATHON - COLUMN REFERENCE\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\\n\")\n",
    "    \n",
    "    if df_enrollment is not None:\n",
    "        f.write(\"ENROLLMENT COLUMNS:\\n\")\n",
    "        f.write(\"-\"*70 + \"\\n\")\n",
    "        for i, col in enumerate(df_enrollment.columns, 1):\n",
    "            f.write(f\"{i:2d}. {col}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "    if df_demographic is not None:\n",
    "        f.write(\"DEMOGRAPHIC COLUMNS:\\n\")\n",
    "        f.write(\"-\"*70 + \"\\n\")\n",
    "        for i, col in enumerate(df_demographic.columns, 1):\n",
    "            f.write(f\"{i:2d}. {col}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "    if df_biometric is not None:\n",
    "        f.write(\"BIOMETRIC COLUMNS:\\n\")\n",
    "        f.write(\"-\"*70 + \"\\n\")\n",
    "        for i, col in enumerate(df_biometric.columns, 1):\n",
    "            f.write(f\"{i:2d}. {col}\\n\")\n",
    "\n",
    "print(f\"\\n Column reference saved: {ref_path}\")\n",
    "print(f\"\\n NEXT STEP: Check the column names above and tell me!\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
